{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t09eeeR5prIJ"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "GCCk8_dHpuNf"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ovpZyIhNIgoq"
   },
   "source": [
    "# Text generation with an RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WGyKZj3bzf9p"
   },
   "source": [
    "### Import TensorFlow and other libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yG_n40gFzf9s"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UHjdCjDuSvX_"
   },
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aavnuByVymwK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 16455 characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(os.path.join('data', 'training.c'), 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Duhg9NrUymwO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int f(int a, int b, int c) {\r\n",
      "\t return a - 58 - b - 74 + c * 40 - c * 34;\r\n",
      "}\r\n",
      "// <end>\r\n",
      "\r\n",
      "\r\n",
      "int f(int a, int c) {\r\n",
      "\t return a - 87 * a + 10 + c - 4 + c + 16;\r\n",
      "}\r\n",
      "// <end>\r\n",
      "\r\n",
      "\r\n",
      "int f(int a, int c) {\r\n",
      "\t return a - 95 * c - 99 + c + 9;\r\n",
      "}\r\n",
      "// <end>\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IlCgQBRVymwR"
   },
   "outputs": [],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rNnrKn_lL-IJ"
   },
   "source": [
    "## Process the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IalZLbvOzf-F"
   },
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bbmsf23Bymwe"
   },
   "source": [
    "### The prediction task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0UHJDA39zf-O"
   },
   "outputs": [],
   "source": [
    "# The maximum length sentence we want for a single input in characters\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n",
    "\n",
    "# Create training examples / targets\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l4hkDU3i7ozi"
   },
   "outputs": [],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9NGu-FkO_kYU"
   },
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MJdfPmdqzf-R"
   },
   "source": [
    "### Create training batches\n",
    "\n",
    "We used `tf.data` to split the text into manageable sequences. But before feeding this data into the model, we need to shuffle the data and pack it into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p2pGotuNzf-S"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r6oUuElIMgVx"
   },
   "source": [
    "## Build The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zHT8cLh7EAsg"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MtCrdfzEI2N0"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wwsrpOik5zhv"
   },
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vPGmAAXmVLGC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           9472      \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    (64, None, 1024)          3938304   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (64, None, 37)            37925     \n",
      "=================================================================\n",
      "Total params: 3,985,701\n",
      "Trainable params: 3,985,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LJL0Q0YPY6Ee"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4HrXTACTdzY-"
   },
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DDl1_Een6rL0"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ieSJdchZggUj"
   },
   "source": [
    "### Configure checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C6XBUUavgF56"
   },
   "source": [
    "Use a `tf.keras.callbacks.ModelCheckpoint` to ensure that checkpoints are saved during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W6fWTriUZP-n"
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = '/Users/miller/Desktop/training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Ky3F_BhgkTW"
   },
   "source": [
    "### Execute the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IxdOA-rgyGvs"
   },
   "source": [
    "To keep training time reasonable, use 10 epochs to train the model. In Colab, set the runtime to GPU for faster training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7yGBE2zxMMHs"
   },
   "outputs": [],
   "source": [
    "EPOCHS=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UK-hmKjYVoll"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2/2 [==============================] - 2s 771ms/step - loss: 3.5805\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 1s 683ms/step - loss: 3.3514\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 2s 826ms/step - loss: 8.2214\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 1s 661ms/step - loss: 3.3093\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 1s 687ms/step - loss: 3.3702\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 1s 663ms/step - loss: 3.3572\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 2s 788ms/step - loss: 3.3211\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 1s 692ms/step - loss: 3.2699\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 1s 733ms/step - loss: 3.2010\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 1s 734ms/step - loss: 3.1141\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 1s 720ms/step - loss: 2.9985\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 1s 697ms/step - loss: 2.8585\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 1s 665ms/step - loss: 2.7051\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 1s 712ms/step - loss: 2.4764\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 1s 696ms/step - loss: 2.3607\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 1s 693ms/step - loss: 2.2798\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 2s 787ms/step - loss: 2.1619\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 1s 709ms/step - loss: 2.0214\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 1s 696ms/step - loss: 1.9137\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 1s 660ms/step - loss: 1.7827\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 2s 815ms/step - loss: 1.6742\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 1s 663ms/step - loss: 1.5554\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 2s 813ms/step - loss: 1.4309\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 1s 673ms/step - loss: 1.3047\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 1s 703ms/step - loss: 1.2016\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 1s 702ms/step - loss: 1.1020\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 1s 678ms/step - loss: 1.0070\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 1s 673ms/step - loss: 0.9378\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 1s 746ms/step - loss: 0.8751\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 2s 752ms/step - loss: 0.8000\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 1s 673ms/step - loss: 0.7613\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 1s 678ms/step - loss: 0.7126\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 1s 683ms/step - loss: 0.6739\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 1s 660ms/step - loss: 0.6242\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 1s 692ms/step - loss: 0.6006\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 1s 668ms/step - loss: 0.5756\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 1s 696ms/step - loss: 0.5603\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 1s 680ms/step - loss: 0.5416\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 2s 807ms/step - loss: 0.5225\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 2s 813ms/step - loss: 0.5114\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 1s 709ms/step - loss: 0.4834\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 2s 800ms/step - loss: 0.4708\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 1s 669ms/step - loss: 0.4504\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 1s 716ms/step - loss: 0.4591\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 1s 698ms/step - loss: 0.4540\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 2s 789ms/step - loss: 0.4445\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 1s 740ms/step - loss: 0.4306\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 1s 714ms/step - loss: 0.4259\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 2s 793ms/step - loss: 0.4328\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 1s 735ms/step - loss: 0.4302\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 2s 804ms/step - loss: 0.4295\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 1s 747ms/step - loss: 0.4162\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 1s 740ms/step - loss: 0.4220\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 1s 725ms/step - loss: 0.4170\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 1s 706ms/step - loss: 0.4115\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 1s 711ms/step - loss: 0.4173\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 1s 725ms/step - loss: 0.4089\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 2s 879ms/step - loss: 0.4113\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 1s 723ms/step - loss: 0.4062\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 2s 753ms/step - loss: 0.4067\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 2s 794ms/step - loss: 0.4067\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 2s 841ms/step - loss: 0.4031\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 2s 761ms/step - loss: 0.4013\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 2s 804ms/step - loss: 0.3963\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 1s 676ms/step - loss: 0.4011\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 1s 670ms/step - loss: 0.3955\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 1s 682ms/step - loss: 0.3950\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 1s 702ms/step - loss: 0.4027\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 1s 697ms/step - loss: 0.3945\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 1s 674ms/step - loss: 0.3897\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 1s 709ms/step - loss: 0.3973\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 1s 725ms/step - loss: 0.3895\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 1s 715ms/step - loss: 0.3906\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 2s 766ms/step - loss: 0.3961\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 2s 806ms/step - loss: 0.3841\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 2s 772ms/step - loss: 0.3863\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 1s 708ms/step - loss: 0.3881\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 2s 783ms/step - loss: 0.3867\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 2s 841ms/step - loss: 0.3839\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 1s 733ms/step - loss: 0.3784\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 1s 735ms/step - loss: 0.3874\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 2s 833ms/step - loss: 0.3872\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 2s 793ms/step - loss: 0.3841\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 2s 843ms/step - loss: 0.3848\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 1s 716ms/step - loss: 0.3806\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 2s 840ms/step - loss: 0.3850\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 2s 757ms/step - loss: 0.3838\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 2s 826ms/step - loss: 0.3800\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 2s 807ms/step - loss: 0.3807\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 2s 811ms/step - loss: 0.3783\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 2s 770ms/step - loss: 0.3747\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 2s 838ms/step - loss: 0.3854\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 1s 742ms/step - loss: 0.3812\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 2s 801ms/step - loss: 0.3789\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 1s 699ms/step - loss: 0.3740\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 1s 720ms/step - loss: 0.3747\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 1s 718ms/step - loss: 0.3732\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 2s 805ms/step - loss: 0.3740\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 1s 736ms/step - loss: 0.3748\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 2s 800ms/step - loss: 0.3765\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kKkD5M6eoSiN"
   },
   "source": [
    "## Generate text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JIPcXllKjkdr"
   },
   "source": [
    "### Restore the latest checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zk2WJ2-XjkGz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/miller/Desktop/training_checkpoints/ckpt_100'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LycQ-ot_jjyu"
   },
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "71xa6jnYVrAN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            9472      \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (1, None, 1024)           3938304   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 37)             37925     \n",
      "=================================================================\n",
      "Total params: 3,985,701\n",
      "Trainable params: 3,985,701\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DjGz1tDkzf-u"
   },
   "source": [
    "### The prediction loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WvuwZBX5Ogfd"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Number of characters to generate\n",
    "    num_generate = 100\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "    # Low temperatures results in more predictable text.\n",
    "    # Higher temperatures results in more surprising text.\n",
    "    # Experiment to find the best setting.\n",
    "    temperature = 1.0\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    state = ''\n",
    "    while state != '<end>':\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # using a categorical distribution to predict the character returned by the model\n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # We pass the predicted character as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "        state += idx2char[predicted_id]\n",
    "        if len(state) > 5:\n",
    "            state = state[1:]\n",
    "\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "text_generation.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}